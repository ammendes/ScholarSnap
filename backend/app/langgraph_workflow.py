
from langgraph.graph import StateGraph, END
from langchain_ollama import OllamaLLM
from app.services.arxiv import fetch_recent_arxiv_papers
from app.services.llm import build_context, get_paper_list
from app.config import OLLAMA_MODEL, BATCH_SIZE


# Define the state for the workflow
class ChatState:
    def __init__(self, user_input):
        self.user_input = user_input    # The raw input from the user
        self.topic = None               # The scientific topic of interest
        self.confirmed = False          # Whether the topic has been confirmed as valid
        self.papers = None              # List of fetched papers
        self.summary = None             # Summary generated by LLM


# Node: Greeting
async def greet_node(state: dict):
    state['greeting'] = "Hello! What scientific topic are you interested in today?"
    return state


# Node: LLM typo check
async def typo_check_node(state: dict):
    
    from app.services.llm import validate_topic_with_llm
    result = await validate_topic_with_llm(state['user_input'])
    print(f"[LLM typo check raw response]: {result}")  # Debug log
    result_lower = result.lower()
    # Only proceed if LLM explicitly says 'valid'
    if result_lower == "valid":
        state['topic'] = state['user_input']
        state['topic_lower'] = state['user_input'].lower()
        state['confirmed'] = True
    elif result_lower.startswith("invalid"):
        state['confirmed'] = False
        state['topic'] = None
        state['topic_lower'] = None
    return state


# Node: RAG pipeline to extract papers
async def rag_node(state: dict):
    papers = await fetch_recent_arxiv_papers(state['topic_lower']) # Fetch papers from arXiv using lowercase
    
    # Enforce batch size limit everywhere
    papers = papers[:BATCH_SIZE]
    print(f"[RAG Node] Number of papers detected after batch size enforcement: {len(papers)}")
    context = build_context(papers) # Structure papers into context
    
    # Download PDFs if papers are found
    if papers:
        from app.services.pdf_utils import download_arxiv_pdfs
        pdf_paths = await download_arxiv_pdfs(papers)
        state['pdf_paths'] = pdf_paths
        # Prompt user after successful download
        if pdf_paths:
            state['processing_prompt'] = (
                f"Download successful for {len(pdf_paths)} papers. Do you want to proceed to processing? (yes/no)"
            )
            state['awaiting_processing_confirmation'] = True
        else:
            state['processing_prompt'] = "No PDFs were downloaded."
            state['awaiting_processing_confirmation'] = False
    else:
        state['pdf_paths'] = []
        state['processing_prompt'] = "No PDFs were downloaded."
        state['awaiting_processing_confirmation'] = False
    
    # Use original topic (not lowercased) for LLM prompt at the end
    summary = await get_paper_list(state['topic'], context)
    state['papers'] = papers
    state['summary'] = summary


    # If awaiting confirmation, interpret user response
    if state.get('awaiting_processing_confirmation') and 'user_response' in state:
        from app.services.llm import classify_processing_response
        decision = await classify_processing_response(state['user_response'])
        print(f"[Processing response classification]: {decision}")
        state['processing_decision'] = decision
        if decision == 'positive':
            state['next_action'] = 'proceed_processing'
            state['awaiting_processing_confirmation'] = False
        elif decision == 'negative':
            # Delete files and inform user
            from app.services.pdf_utils import cleanup_tmp_folder
            cleanup_tmp_folder()
            state['next_action'] = 'stop_and_cleanup'
            state['processing_prompt'] = "Ok, we'll stop here. Downloaded files have been deleted."
            state['awaiting_processing_confirmation'] = False
        else:
            state['next_action'] = 'clarify_processing'
            state['processing_prompt'] = "Sorry, I did not understand that. Could you be more specific? Do you want to process the downloaded files? (yes/no)"
    return state


# Node: End/Output
async def end_node(state: dict):
    papers = state.get("papers")
    summary = state.get("summary")
    topic = state.get("topic")
    greeting = state.get("greeting")

    # Handle user declining processing
    if state.get("next_action") == "stop_and_cleanup":
        return {
            "greeting": greeting,
            "topic": None,
            "summary": None,
            "papers": None,
            "paper_list": None,
            "no_papers": None,
            "clarification": None,
            "processing_prompt": "Sure, maybe next time!"
        }

    if papers is None:
        # No RAG step was run (invalid topic or session start)
        clarification = None
        if topic is None:
            clarification = "Invalid scientific topic. Please try a different one."
        return {
            "greeting": greeting,
            "topic": topic,
            "summary": None,
            "papers": None,
            "paper_list": None,
            "no_papers": None,
            "clarification": clarification,
        }
    elif len(papers) == 0:
        # RAG ran, but no papers found
        return {
            "greeting": greeting,
            "topic": topic,
            "summary": None,
            "papers": [],
            "paper_list": None,
            "no_papers": "No papers found for this topic."
        }
    else:
        # Papers found, format bullet list
        paper_list = "\n".join([
            f"- {p['title']} (Authors: {', '.join(p['authors'])})" for p in papers
        ])
        return {
            "greeting": greeting,
            "topic": topic,
            "summary": summary,
            "papers": papers,
            "paper_list": paper_list,
            "no_papers": None,
        }


# Define workflow
workflow = StateGraph(dict) 

workflow.set_entry_point("greet")
workflow.add_node("greet", greet_node) # Start with greeting node
workflow.add_node("typo_check", typo_check_node) # 

# typo_check: valid -> rag, invalid -> end (for re-ask)
workflow.add_conditional_edges(
    "typo_check",
    lambda state: "rag" if state["confirmed"] else "end"
)

# Add new nodes for processing confirmation
workflow.add_node("rag", rag_node)
workflow.add_node("end", end_node)

# Entry: greet -> typo_check
workflow.add_edge("greet", "typo_check")





workflow.set_finish_point("end")

langgraph_app = workflow.compile()
