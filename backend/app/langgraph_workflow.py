from langgraph.graph import StateGraph, END
from langchain_ollama import OllamaLLM
from app.services.arxiv import fetch_recent_arxiv_papers
from app.services.llm import build_context, get_paper_list
from app.config import OLLAMA_MODEL

# Define the state for the workflow
class ChatState:
    def __init__(self, user_input):
        self.user_input = user_input    # The raw input from the user
        self.topic = None               # The scientific topic of interest
        self.confirmed = False          # Whether the topic has been confirmed as valid
        self.papers = None              # List of fetched papers
        self.summary = None             # Summary generated by LLM


# Node: Greeting
async def greet_node(state: dict):
    state['greeting'] = "Hello! What scientific topic are you interested in today?"
    return state


# Node: LLM typo check
async def typo_check_node(state: dict):
    
    # Use Ollama to check if the input is a valid scientific topic
    llm = OllamaLLM(model=OLLAMA_MODEL) # Define LLM instance
    prompt = (                          # Define prompt
        "You are a scientific assistant. "
        "Your task is to determine if the given topic is a valid scientific research topic. "
        "Anatomical, medical, biological, physical, chemical, computational, and engineering topics are all valid. "
        "When checking if a topic is valid, ignore capitalization/case. "
        "If the topic is valid, reply ONLY with: \"Valid\" "
        "If the topic is invalid, reply ONLY with: \"Invalid scientific topic. Please try a different one.\" "
        f"\nTopic: {state['user_input']}"
    )
    result = llm.invoke(prompt).strip() # Get LLM response
    print(f"[LLM typo check raw response]: {result}")  # Debug log
    result_lower = result.lower() # Normalize to all lowercase for comparison

    # Only proceed if LLM explicitly says 'valid'
    if result_lower == "valid":
        state['topic'] = state['user_input']  # Preserve original user input
        state['topic_lower'] = state['user_input'].lower()  # Use lowercase for all internal steps
        state['confirmed'] = True
    elif result_lower.startswith("invalid"):
        state['confirmed'] = False
        state['topic'] = None
        state['topic_lower'] = None
    return state


# Node: RAG pipeline to extract papers
async def rag_node(state: dict):
    papers = await fetch_recent_arxiv_papers(state['topic_lower']) # Fetch papers from arXiv using lowercase
    context = build_context(papers) # Structure papers into context
    # Use original topic (not lowercased) for LLM prompt at the end
    summary = await get_paper_list(state['topic'], context)
    state['papers'] = papers
    state['summary'] = summary
    return state


# Node: End/Output
async def end_node(state: dict):
    papers = state.get("papers")
    summary = state.get("summary")
    topic = state.get("topic")
    greeting = state.get("greeting")

    if papers is None:
        # No RAG step was run (invalid topic or session start)
        clarification = None
        if topic is None:
            clarification = "Invalid scientific topic. Please try a different one."
        return {
            "greeting": greeting,
            "topic": topic,
            "summary": None,
            "papers": None,
            "paper_list": None,
            "no_papers": None,
            "clarification": clarification,
        }
    elif len(papers) == 0:
        # RAG ran, but no papers found
        return {
            "greeting": greeting,
            "topic": topic,
            "summary": None,
            "papers": [],
            "paper_list": None,
            "no_papers": "No papers found for this topic."
        }
    else:
        # Papers found, format bullet list
        paper_list = "\n".join([
            f"- {p['title']} (Authors: {', '.join(p['authors'])})" for p in papers
        ])
        return {
            "greeting": greeting,
            "topic": topic,
            "summary": summary,
            "papers": papers,
            "paper_list": paper_list,
            "no_papers": None,
        }


# Define workflow
workflow = StateGraph(dict) 

workflow.set_entry_point("greet")
workflow.add_node("greet", greet_node) # Start with greeting node
workflow.add_node("typo_check", typo_check_node) # 

# typo_check: valid -> rag, invalid -> end (for re-ask)
workflow.add_conditional_edges(
    "typo_check",
    lambda state: "rag" if state["confirmed"] else "end"
)
workflow.add_node("rag", rag_node)
workflow.add_node("end", end_node)

# Entry: greet -> typo_check
workflow.add_edge("greet", "typo_check")



# rag: if papers found -> end, else -> end (handle in end_node)
workflow.add_edge("rag", "end")

workflow.set_finish_point("end")

langgraph_app = workflow.compile()
